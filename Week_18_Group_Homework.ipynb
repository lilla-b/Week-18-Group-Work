{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 18 In Class Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Look up the Adam optimization functions in PyTorch https://pytorch.org/docs/stable/optim.html . \n",
    "How does it work? Try at least one other optimization function with the diabetes dataset shown in class. How does the model perform with the new optimizer? Did it perform better or worse than Adam? Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam is a gradient descent optimization algorithm. All optimization algorithms seek to find the input for a target function that minimizes (or maximizes) output. Some optimization algorithms require that their objective (loss) functions are differentiable at a given point (e.g., a proposed solution). Within this set, some algorithms are based on using the first derivative; for multivariate functions, the \"derivitive\" is the gradient. \"First order\" algorithms use the gradient to find the direction to move in the search space, with the goal of getting as close to the optimum as possible. For each step, the derivative is computed, and, if the minimum is sought, a step (a parameter, the learning rate) is taken downhill, in the direction OPPOSITE to the gradient. This is the gradient descent. Stochastic gradient descents (SGDs) operate similarly, except that the gradient is not explicitly computed. Instead, it is obtained from prediction error in the training data set. \n",
    "\n",
    "The chief difference between Adam and earlier SGDs is that the learning rate is adaptive, not fixed. My understanding is that Adam adapts the learning rate based on the first and second moments of the gradient (mean and variance). The consequence of this is that it performs well in situations that earlier SGD algorithms had some difficulty with, in particular, problems where the underlying values change over time (non-stationary) or sparse gradients  (a very weak \"signal\", i.e. a lot of small numbers that don't have enough information to update the weights of the network). Furthermore, Adam is computationally efficient, requires little memory, and is good for problems with lots of data and parameters.\n",
    "\n",
    "I used four optimiation algorithms: Adam, its two \"ancestral\" algorithsms, Adagrad and RMSprop, and Rprop (resilient backpropagation). Here are the performance results. The loss score is for the final epoch (491). \n",
    "Adam:  loss score = 0.0046, accuracy score = 0.69 \n",
    "UAdagrad: loss score = 0.3084, accuracy score = 0.70. \n",
    "RMSprop: loss score = 0.0555, accuracy score = 0.73. \n",
    "Rprop algorithm: loss score = 0.0043, accuracy score = 0.71.\n",
    "\n",
    "Rprop had the smallest loss score and the second smallest accuracy score; RMSprop had the highest accuracy score. Adam had the second lowest loss scorr and the lowest accuracy score. To be honest, I am not sure why the performances are what they are, but Adam outperformed its parent algorithms in terms of minimizing the loss score. This is probably due to the fact that it combines the adaptive properties of both. I can't speak to the accuracy. They seem close enough to one another, but I don't know how to compare them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "diabetes_df = pd.read_csv(\"../week_13/diabetes.csv\")\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = diabetes_df.drop('Outcome', axis=1).values\n",
    "y = diabetes_df['Outcome'].values\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)\n",
    "\n",
    "# #Standardize\n",
    "sc= StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8514, -0.9801, -0.4048,  ..., -0.6077,  0.3108, -0.7922],\n",
      "        [ 0.3566,  0.1614,  0.4654,  ..., -0.3021, -0.1164,  0.5610],\n",
      "        [-0.5494, -0.5045, -0.6223,  ...,  0.3726, -0.7649, -0.7076],\n",
      "        ...,\n",
      "        [-0.8514, -0.7582,  0.0303,  ...,  0.7800, -0.7861, -0.2847],\n",
      "        [ 1.8665, -0.3142,  0.0303,  ..., -0.5695, -1.0194,  0.5610],\n",
      "        [ 0.0546,  0.7322, -0.6223,  ..., -0.3149, -0.5770,  0.3073]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F #this has activation functions\n",
    "\n",
    "# Creating tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_Model(nn.Module):\n",
    "    def __init__(self, input_features=8, hidden1=20, hidden2=20, out_features =2):\n",
    "        super().__init__()\n",
    "        self.layer_1_connection = nn.Linear(input_features, hidden1)\n",
    "        self.layer_2_connection = nn.Linear(hidden1, hidden2)\n",
    "        self.out = nn.Linear(hidden2, out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #apply activation functions\n",
    "        x = F.relu(self.layer_1_connection(x))\n",
    "        x = F.relu(self.layer_2_connection(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "#instantiate the model\n",
    "model = ANN_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.Rprop(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1 with loss: 0.6474142670631409\n",
      "Epoch number: 11 with loss: 0.45198854804039\n",
      "Epoch number: 21 with loss: 0.3870345950126648\n",
      "Epoch number: 31 with loss: 0.3395462930202484\n",
      "Epoch number: 41 with loss: 0.30658769607543945\n",
      "Epoch number: 51 with loss: 0.2752005457878113\n",
      "Epoch number: 61 with loss: 0.25215259194374084\n",
      "Epoch number: 71 with loss: 0.22651627659797668\n",
      "Epoch number: 81 with loss: 0.20444045960903168\n",
      "Epoch number: 91 with loss: 0.18579360842704773\n",
      "Epoch number: 101 with loss: 0.17048318684101105\n",
      "Epoch number: 111 with loss: 0.1541174203157425\n",
      "Epoch number: 121 with loss: 0.13827119767665863\n",
      "Epoch number: 131 with loss: 0.12396775931119919\n",
      "Epoch number: 141 with loss: 0.11416137963533401\n",
      "Epoch number: 151 with loss: 0.10690709203481674\n",
      "Epoch number: 161 with loss: 0.10016005486249924\n",
      "Epoch number: 171 with loss: 0.09251974523067474\n",
      "Epoch number: 181 with loss: 0.08521822094917297\n",
      "Epoch number: 191 with loss: 0.07992039620876312\n",
      "Epoch number: 201 with loss: 0.07561808824539185\n",
      "Epoch number: 211 with loss: 0.0714590921998024\n",
      "Epoch number: 221 with loss: 0.06749682128429413\n",
      "Epoch number: 231 with loss: 0.06302600353956223\n",
      "Epoch number: 241 with loss: 0.05860539898276329\n",
      "Epoch number: 251 with loss: 0.05476570874452591\n",
      "Epoch number: 261 with loss: 0.05172470211982727\n",
      "Epoch number: 271 with loss: 0.04877886921167374\n",
      "Epoch number: 281 with loss: 0.04568939283490181\n",
      "Epoch number: 291 with loss: 0.04181555658578873\n",
      "Epoch number: 301 with loss: 0.038556020706892014\n",
      "Epoch number: 311 with loss: 0.035074371844530106\n",
      "Epoch number: 321 with loss: 0.03198758885264397\n",
      "Epoch number: 331 with loss: 0.028254643082618713\n",
      "Epoch number: 341 with loss: 0.025271661579608917\n",
      "Epoch number: 351 with loss: 0.02185543067753315\n",
      "Epoch number: 361 with loss: 0.01940494030714035\n",
      "Epoch number: 371 with loss: 0.017250988632440567\n",
      "Epoch number: 381 with loss: 0.015423562377691269\n",
      "Epoch number: 391 with loss: 0.013328447937965393\n",
      "Epoch number: 401 with loss: 0.012059723027050495\n",
      "Epoch number: 411 with loss: 0.010907444171607494\n",
      "Epoch number: 421 with loss: 0.009213805198669434\n",
      "Epoch number: 431 with loss: 0.008498825132846832\n",
      "Epoch number: 441 with loss: 0.007603113539516926\n",
      "Epoch number: 451 with loss: 0.0067962100729346275\n",
      "Epoch number: 461 with loss: 0.0058118379674851894\n",
      "Epoch number: 471 with loss: 0.005462081637233496\n",
      "Epoch number: 481 with loss: 0.004850609693676233\n",
      "Epoch number: 491 with loss: 0.004323161207139492\n"
     ]
    }
   ],
   "source": [
    "#run model through multiple epochs/iterations\n",
    "final_loss = []\n",
    "n_epochs = 500\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = loss_function(y_pred, y_train)\n",
    "    final_loss.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 1:\n",
    "        print(f'Epoch number: {epoch} with loss: {loss.item()}')\n",
    "    \n",
    "    optimizer.zero_grad() #zero the gradient before running backwards propagation\n",
    "    loss.backward() #for backward propagation \n",
    "    optimizer.step() #performs one optimization step each epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(X_test):\n",
    "        prediction = model(data)\n",
    "        y_pred.append(prediction.argmax().item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "a_score = accuracy_score(y_test, y_pred)\n",
    "print(a_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.77      0.78       100\n",
      "           1       0.59      0.61      0.60        54\n",
      "\n",
      "    accuracy                           0.71       154\n",
      "   macro avg       0.69      0.69      0.69       154\n",
      "weighted avg       0.72      0.71      0.72       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Write a function that lists and counts the number of divisors for an input value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1:\n",
    "Input: 5\n",
    "Output: “There are 2 divisors: 1 and 5”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2:\n",
    "Input: 40\n",
    "Output: “There are 8 divisors: 1, 2, 4, 5, 8, 10, 20, and 40”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Create a function that accepts an array of names and returns a string formatted as a list of names separated by commas EXCEPT for the last two names, which are separated by an ampersand (and sign - &)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
